# -*- coding: utf-8 -*-
"""project Text Mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lQ5wuIkkvNqbec5G74bcwERGZrvlUjfl
"""

!git clone https://github.com/fabjonvince/president_speech

"""# R

## 1 Load library
"""

pip install rpy2==3.5.1

# Commented out IPython magic to ensure Python compatibility.
# %load_ext rpy2.ipython

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# install.packages("lsa")
# install.packages("tm")
# install.packages("dplyr")
# install.packages("tidytext")
# install.packages("simEd")
# install.packages("stats")
# install.packages('ggplot2')
# install.packages('ggthemes')
# install.packages('ggrepel')
# install.packages('RColorBrewer')
# install.packages('corrplot')
# install.packages("gridExtra")
# install.packages('wordcloud')
# install.packages('base')
# install.packages('cluster')
# 
# 
# library(lsa)
# library(tm)
# library(dplyr)
# library(tidytext)
# library(simEd)
# library(stats)
# library(ggplot2)
# library(ggthemes)
# library(ggrepel)
# library(RColorBrewer)
# library(corrplot)
# library(gridExtra)
# library(wordcloud)
# library(base)
# library(cluster)

"""## 2 Prepare Corpus

load the files

remove capital letter, numbers, punctation, stopwords and whitespace
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# docs<-Corpus(DirSource("president_speech/speech"))
# docs_clean <- docs %>%
#   tm_map(tolower) %>%
#   tm_map(removeNumbers) %>%
#   tm_map(removePunctuation, ucp=TRUE) %>%
#   tm_map(removeWords,tidytext::stop_words$word) %>%
#   tm_map(trimws) %>%
#   tm_map(stripWhitespace)
# m_docs_clean<-TermDocumentMatrix(docs_clean)

"""## 3 Cluster

create LSA space, dimensionality calculated with a methods that compute a 'good' number of singular values
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# lsa_out <- lsa::lsa(m_docs_clean, dims = lsa::dimcalc_share())
# docs_mat<- lsa_out$dk[,c(1:2)]
# docs_mat_df<- as.data.frame(docs_mat)
# colnames(docs_mat_df)<-c("dim1", "dim2")
#

# Commented out IPython magic to ensure Python compatibility.
# %%R
# # set a seed
# set.seed(1234)
#

"""Set 5 clusters because there are 5 different presidents"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# clus<-kmeans(docs_mat_df,5)
# docs_mat_df$cluster<-factor(clus$cluster)
# 
# ggplot(data=docs_mat_df,aes(x=dim1,y=dim2)) +
#   geom_point(aes(color=cluster),size=2) +
#   geom_abline(slope=-4,intercept=-.55,alpha=.5,lty=2) +
#   geom_label_repel(aes(label=rownames(docs_mat_df)),data=docs_mat_df,size=2.5) +
#   theme_fivethirtyeight() +
#   scale_color_brewer(palette='Set1')
# 
#

"""Draw the dashline without applying any machine learning algorithm namely by observation of a clusteritation by party

Obama's speeches are pretty well clustered

Clinton 1993, 1994, 1995  are well clustered

Bush son and father are close

## 4 Word analysis

### 4.1 code

Corpus filtering as in Section 2

retrieve the president's name, party and year from the name of the file

append everything in a dataframe
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# makeFrequencyWord<-function(df){
#   temp<-Corpus(VectorSource(df[,]))
#   temp_clean <- temp %>%
#   tm_map(tolower) %>%
#   tm_map(removeNumbers) %>%
#   tm_map(removePunctuation, ucp=TRUE) %>%
#   tm_map(removeWords,tidytext::stop_words$word) %>%
#   tm_map(trimws) %>%
#   tm_map(stripWhitespace)
# 
#   word_Vec<-c()
#   for (i in 1:length(temp_clean)){
#     current<-sort(strsplit(as.character(temp_clean[[i]]$content),' ')[[1]])
#     word_Vec<-append(word_Vec,current,length(current))
#   }
#     return(word_Vec)
# }

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# files <- list.files(path="president_speech/speech", pattern="*.txt", full.names=T, recursive=FALSE)
# listAll<-list()
# for(i in 1:length(files)){
#     dat<-as.data.frame(readLines(files[i]),stringsAsFactors=F)
#     president<-strsplit(gsub("president_speech/speech/|.txt","",files[i]),'_')[[1]][1]
#     year<-as.numeric(strsplit(gsub("president_speech/speech/|.txt","",files[i]),'_')[[1]][2])
# 
#     dat_words<-makeFrequencyWord(dat)
# 
#     dat_df<-data.frame('word'= dat_words)
#     dat_df$year<-rep.int(year,nrow(dat_df))
# 
#     if(president=='Bush'){
#         if(year<2001){
#             president<-paste0(president,'-Sr')
#             }
#             else {
#                 president<-paste0(president,'-Jr')
#             }
#         }
# 
#     dat_df$president<-rep(president,nrow(dat_df))
#     if(president %in% c('Bush-Sr','Bush-Jr','Trump')){
#         dat_df$party<-rep('Republican',nrow(dat_df))
#     }
#     else{
#             dat_df$party<-rep('Democrat',nrow(dat_df))
#         }
#     listAll[[i]]<-dat_df
# }
# 
# PPY <- do.call("rbind", listAll)

"""### 4.2 Distribution of the 30 most frequent words"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# presidents<-c('Bush-Sr','Clinton','Bush-Jr','Obama','Trump')
# wordPresident<-list()
# 
# for(i in 1:length(presidents)){
#   col<-ifelse(i %in% c(2,4),'Blues','Reds')
#   wordPresident[[i]]<-PPY %>%
#     filter(president==presidents[i]) %>%
#     group_by(word) %>%
#     summarise(count=n()) %>%
#     top_n(30) %>%
#     ggplot(aes(x=reorder(word,count),y=count,fill=count)) +
#     geom_histogram(stat='identity') +
#     coord_flip() +
#     theme_fivethirtyeight() +
#     ggtitle(paste0('Most frequent words used by ',presidents[i])) +
#     theme(legend.position='none',
#           plot.title = element_text(size =10),
#           axis.text = element_text(size=10)) +
#     scale_fill_gradientn(colors=brewer.pal(2,col))
# 
# }
# 
#

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# for(i in seq(from=0, to=length(presidents), by=2)){
#   do.call(grid.arrange, c(wordPresident[i], wordPresident[i+1], ncol=2))
# }

"""### 4.3 Word Clouds"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# par(mfrow=c(3, 2),bg="black")
# for(i in 1:length(presidents)){
#     tdm <- TermDocumentMatrix(Corpus(VectorSource((PPY %>% filter(president==presidents[i]) %>% select(word))$word)))
#     m_tdm<-as.matrix(tdm)
#     word.freq<-sort(rowSums(m_tdm), decreasing=T)
#     col<-ifelse(i %in% c(2,4),'Blues','Reds')
#     pal<-rev(brewer.pal(10,col))
#     wordcloud(words=names(word.freq),freq = word.freq,random.order=F,colors=pal, max.words=200,scale=c(3,1))
#     title(paste0('Most frequent words used by ',presidents[i]),col.main='#EBCC2A',cex.main=1.25)
# }

"""## 5 Count vs Time

From wordcloud we can see that a word can have variations
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# PPY %>% group_by(word) %>% summarize(count=n()) %>% filter(grepl("^america",word))

"""So we can search for a group of words, instead that 1 word and miss the others"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# #define rectangle for colors
# start_r<-c(1988.5,2000.5,2016.5)
# end_r<-c(1992.5,2008.5,2017.5)
# dates_r<-data.frame(xmin=start_r,xmax=end_r,ymin=0,ymax=Inf)
# 
# start_b<-c(1992.5,2008.5)
# end_b<-c(2000.5,2016.5)
# dates_b<-data.frame(xmin=start_b,xmax=end_b,ymin=0,ymax=Inf)
# 
# colfunc <- colorRampPalette(c("black", "gray"))

"""### 5.1 AMERICA  and variant"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# am<-c('america','american','americas','americans')
# tt<-data.frame(word=character(), year=integer(), president = character(), party=character())
# for(j in 1:length(listAll)){
#   temp<-listAll[[j]] %>% filter(word %in% am)
#   tt<-rbind(tt,temp)
# }

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# tt %>% group_by(year,word) %>%
#   summarize(count=n()) %>%
#   ggplot() + geom_bar(aes(x=year,y=count,fill=word),stat='identity',alpha=1) +
#   theme_fivethirtyeight() +
#   scale_fill_manual(name="",values = colfunc(length(unique(tt$word)))) +
#   geom_rect(data = dates_r, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), alpha = 0.3,fill='red') +
#   geom_rect(data = dates_b, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), alpha = 0.3,fill='blue') +
#   ggtitle('Counts of \"America\" and its variants')

"""### 5.2 TAX and its variant"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# alltax<-c('tax','taxes','taxpayer','taxpayers')
# tt<-data.frame(word=character(), year=integer(), president = character(), party=character())
# for(j in 1:length(listAll)){
#   temp<-listAll[[j]] %>% filter(word %in% alltax)
#   tt<-rbind(tt,temp)
# }

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# tt %>% group_by(year,word) %>%
#   summarize(count=n()) %>%
#   ggplot() + geom_bar(aes(x=year,y=count,fill=word),stat='identity',alpha=1) +
#   theme_fivethirtyeight() +
#   scale_fill_manual(name="",values = colfunc(length(unique(tt$word)))) +
#   geom_rect(data = dates_r, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), alpha = 0.3,fill='red') +
#   geom_rect(data = dates_b, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), alpha = 0.3,fill='blue') +
#   ggtitle('Counts of \"Tax\" and its variants')

"""### 5.3 JOB and its variants"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# alljob<-c('job','jobs')
# tt<-data.frame(word=character(), year=integer(), president = character(), party=character())
# for(j in 1:length(listAll)){
#   temp<-listAll[[j]] %>% filter(word %in% alljob)
#   tt<-rbind(tt,temp)
# }

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# tt %>% group_by(year,word) %>%
#   summarize(count=n()) %>%
#   ggplot() + geom_bar(aes(x=year,y=count,fill=word),stat='identity',alpha=1) +
#   theme_fivethirtyeight() +
#   scale_fill_manual(name="",values = colfunc(length(unique(tt$word)))) +
#   geom_rect(data = dates_r, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), alpha = 0.3,fill='red') +
#   geom_rect(data = dates_b, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), alpha = 0.3,fill='blue') +
#   ggtitle('Counts of \"Job\" and its variants')

"""### 5.4 Results

AMERICA and its variant it's more used by Democrats

TAX is also mostly Democrats and the falls during the first Clinton and Bush Jr from around 30 to 10 are strange

JOB is theme more addressed by Democrats than Republican too

It's interesting to see the change each time Democrats and Republicans switch

## 6 Error

Total within-cluster sum of square
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# clus$tot.withinss

"""Silhoutte index

how well object lies within its cluster
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# ss <- silhouette(clus$cluster, dist(docs_mat_df))
# mean(ss[,3])

"""# Machine Learning

## 1 Load Data
"""

import numpy as np
import pandas as pd
import os
random_seed = 42
np.random.seed(random_seed)

path='president_speech/speech'
dirs = os.listdir(path)
df = pd.DataFrame(columns=['Year', 'President', 'Text', 'Party'])

# for every file in the dir, take the filename, save the President, the year and the speech (already lower case)
for i in range(len(dirs)):
  filename = dirs[i].split('_')
  if len(filename) == 2:
    name = filename[0]
    year = filename[1].split('.')[0]
    df.loc[i,'Year'] = year
    df.loc[i,'President'] = name

    files = os.path.join(path, dirs[i])
    text_file = open(files, 'r')

    lines = text_file.read()
    df.loc[i, 'Text'] = lines.replace('\n', '').lower()

df.reset_index(drop=True, inplace=True)
df.Year = df.Year.astype(int)
df.President = df.President.astype(str)
df.Text = df.Text.astype(str)

df.shape

df.head()

"""## 2 Prepare Data"""

import en_core_web_sm
nlp = en_core_web_sm.load() #english pipeline optimized for CPU
from spacy.lang.en.stop_words import STOP_WORDS

# set 2 different name for Bush Senior and Junior
indices = df.query('President=="Bush" & Year<2001').index
df.loc[indices,'President'] = 'Bush-Sr'

indices = df.query('President=="Bush" & Year>2000').index
df.loc[indices,'President'] = 'Bush-Jr'

# add the party of the President

def pres_to_party(name):

  if name in ('Bush-Jr', 'Bush-Sr', 'Trump'):
    return 'Republican'
  else:
    return 'Democratic'

df.Party = df.President.apply(pres_to_party)

df.groupby('Party').size()

# clean the text and retrieve the single word, then apply lemmatization to the words
df['Clean'] = df['Text'].str.replace(r"[^A-Za-z]", " ")
df['Clean'] = df['Clean'].apply(lambda x: nlp(x))
df['Clean'] = df['Clean'].apply(lambda x: [token.lemma_ for token in x if token.text not in STOP_WORDS])

# join the tokens to build up the document cleaned
df["Clean"] = [" ".join(x) for x in df['Clean']]
df.head()

"""## 3 Cluster"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# apply the TF-IDF transformation from the column clean_document

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['Clean'])

X = X.toarray()
print(X.shape)

# Create a dataframe containing the result from TF-IDF

X_df = pd.DataFrame(X,
             columns=vectorizer.get_feature_names_out(),
             index=[x for x in range(df.shape[0])])

X_df.head()
# each line correspond to a President speech
# each column is a word in the vocabulary
# each cell is the TF-IDF score

# Clustering on documents with KMeans, 5 cluster as the number of President
clustering = KMeans(n_clusters=5, random_state=random_seed)

# Fit on data
# No need to normalize data, it already is due to TF-IDF
clustering.fit(X)

# Write cluster ids into corpus and X_df
df['cluster_id'] = clustering.labels_
display(df.head())
X_df['cluster_id'] = clustering.labels_
display(X_df.head())

# Number of documents in each cluster
df['cluster_id'].value_counts()

# Principal Component Analysis which reduces the dimensionality of a data to an arbitrary number while preserving most of the information contained in it
pca = PCA(n_components=2, random_state=random_seed)

pca_vecs = pca.fit_transform(X)
pca_center = pca.fit_transform(clustering.cluster_centers_)

# plot the document and the center

LABEL_COLOR_MAP ={0: 'blue',
                  1: 'green',
                  2: 'yellow',
                  3: 'purple',
                  4: 'black'}

label_color = [LABEL_COLOR_MAP[l]  for l in clustering.labels_]

plt.scatter(pca_vecs[:,0], pca_vecs[:,1], c=label_color)
plt.scatter(pca_center[:,0], pca_center[:,1], c='red')
plt.show()

"""## 4 Word analysis"""

import wordcloud

# 5 Most frequent words in each cluster

cols = [c for c in X_df.columns if c!='cluster_id']

for c in df['cluster_id'].value_counts().index:
    print("CLUSTER ", c)
    print(X_df.loc[X_df['cluster_id']==c,cols].mean(axis=0).sort_values(ascending=False)[0:5])
    print('-----------')

# Word cloud for the 5 clusters
wd = wordcloud.WordCloud()
for c in df['cluster_id'].value_counts().index:
    print("CLUSTER ", c)
    texts = " ".join(df.loc[df['cluster_id']==c,'Clean'])
    cloud = wd.generate(texts)
    plt.imshow(cloud)
    plt.show()
    print('-----------')

"""## 5  Count vs Time

From wordcloud we can see that a word can have variations
"""

print(np.unique(list(filter(lambda x: x.startswith("americ"), df.Clean[0].split()))))

"""So we can search for a group of words, instead that 1 word and miss the others"""

# prepare the dataset with the count of the words for year

df_cont = pd.DataFrame()
df_cont['Year'] = df['Year']
df_cont['cont'] = 0
df_cont.cont = df_cont.cont.astype(int)
df_cont.head()

# fo every document assign the color of the party

colorsValue = []
for yy in df_cont.Year:
  if (yy > 1988 and yy < 1993) or (yy > 2000 and yy < 2009) or (yy > 2016):
    colorsValue.append('red')
  else:
    colorsValue.append('blue')

"""### 5.1 AMERICA and its variants"""

# count the occurrences of the words in the speech

def count_words(words):

  for i in range(df.shape[0]):
    cont = 0
    for w in words:
      cont += df.loc[i, 'Clean'].count(w)
    year = df.loc[i, 'Year']
    df_cont.cont[df_cont.Year== year] = int(cont)

americas = ['america', 'americas', 'american', 'americans']
count_words(americas)
df_cont.head()

plt.bar(x=df_cont.Year, height=df_cont.cont, color=colorsValue)
plt.xlabel('Year')
plt.ylabel('No of occurences')
plt.show()

"""### 5.2 TAX and its variants"""

taxes = ['tax','taxes','taxpayer','taxpayers']
count_words(taxes)
df_cont.head()

plt.bar(x=df_cont.Year, height=df_cont.cont, color=colorsValue)
plt.xlabel('Year')
plt.ylabel('No of occurences')
plt.show()

"""### 5.3 JOB and its variants"""

jobs = ['job', 'jobs']
count_words(jobs)
df_cont.head()

plt.bar(x=df_cont.Year, height=df_cont.cont, color=colorsValue)
plt.xlabel('Year')
plt.ylabel('No of occurences')
plt.show()

"""### 5.4 Result

Like in the R version

AMERICA and its variant it's more used by Democrats

TAX is also mostly Democrats and the falls during the first Clinton and Bush Jr from around 30 to 10 are strange

JOB is theme more addressed by Democrats than Republican too

It's interesting to see the change each time Democrats and Republicans switch

## 6 Error
"""

from sklearn.metrics import silhouette_score

"""Tota sum of square error"""

clustering.inertia_

"""Silhouette index"""

silhouette_ml = silhouette_score(X, clustering.labels_, metric='euclidean')
print(silhouette_ml)

"""We can see that SSE and silhouette index are much worse than the one computed in R

# GNN

## 1 Load Data
"""

import numpy as np
import pandas as pd
import os
random_seed = 42
np.random.seed(random_seed)

pip install torch

pip install torch-scatter torch-sparse torch-geometric -f https://data.pyg.org/whl/torch-2.1.0+cpu.html

path='president_speech/speech'
dirs = os.listdir(path)

df_gnn = pd.DataFrame(columns=['Year', 'President', 'Text', 'Party'])

# for every file in the dir, take the filename, save the President, the year and the speech (already lower case)
for i in range(len(dirs)):
  filename = dirs[i].split('_')
  if len(filename) == 2:
    name = filename[0]
    year = filename[1].split('.')[0]
    df_gnn.loc[i,'Year'] = year
    df_gnn.loc[i,'President'] = name

    files = os.path.join(path, dirs[i])
    text_file = open(files, 'r')

    lines = text_file.read()
    df_gnn.loc[i, 'Text'] = lines.replace('\n', '').lower()

df_gnn.reset_index(drop=True, inplace=True)
df_gnn.Year = df_gnn.Year.astype(int)
df_gnn.President = df_gnn.President.astype(str)
df_gnn.Text = df_gnn.Text.astype(str)

df_gnn.head()

"""## 2 Prepare Data"""

import en_core_web_sm
nlp = en_core_web_sm.load() #english pipeline optimized for CPU
from spacy.lang.en.stop_words import STOP_WORDS
#import torch

# set 2 different name for Bush Senior and Junior
indices = df_gnn.query('President=="Bush" & Year<2001').index
df_gnn.loc[indices,'President'] = 'Bush-Sr'

indices = df_gnn.query('President=="Bush" & Year>2000').index
df_gnn.loc[indices,'President'] = 'Bush-Jr'

# add the party of the President

def pres_to_party(name):

  if name in ('Bush-Jr', 'Bush-Sr', 'Trump'):
    return 'Republican'
  else:
    return 'Democratic'

df_gnn.Party = df_gnn.President.apply(pres_to_party)

df_gnn.groupby('Party').size()

# clean the text and retrieve the single word, then apply lemmatization to the words
df_gnn['Clean'] = df_gnn['Text'].str.replace(r"[^A-Za-z]", " ")
df_gnn['Clean'] = df_gnn['Clean'].apply(lambda x: nlp(x))
df_gnn['Clean'] = df_gnn['Clean'].apply(lambda x: [token.lemma_ for token in x if token.text not in STOP_WORDS])

# join the tokens to build up the document cleaned
df_gnn["Clean"] = [" ".join(x) for x in df_gnn['Clean']]
df_gnn.head()

"""## 3 Cluster"""

pip install dgl==0.9.1

import dgl
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
import itertools
from scipy.stats import pearsonr

# apply the TF-IDF transformation of the column Clean
vectorizer = TfidfVectorizer(stop_words='english')
X_vect = vectorizer.fit_transform(df_gnn['Clean'])

X_vect = X_vect.toarray()
print(X_vect.shape)

# Create a dataframe containing the result from TF-IDF

X_df_gnn = pd.DataFrame(X_vect,
             columns=vectorizer.get_feature_names_out(),
             index=[x for x in range(df_gnn.shape[0])],dtype=np.float64)

X_df_gnn.head()
# each line correspond to a President speech
# each column is a word in the vocabulary
# each cell is the TF-IDF score

# create a copy of the TF-IDF dataframe where the value are 0 or 1
X_df_gnn2 = X_df_gnn.copy()
X_df_gnn2[X_df_gnn2 > 0] = 1
X_df_gnn2 = X_df_gnn2.astype(np.int32)

# I will use the new df as the edges of the graph
# I will consider an edge between 2 documents if they have the same word

words = X_df_gnn2.columns
all_edges = np.array([], dtype=np.int32).reshape((0,2))
for w in words:
  df_words = X_df_gnn2[w]
  ww = df_words[df_words == 1].index
  #build all combinations
  permutations = list(itertools.combinations(ww, 2))
  edges_source = [e[0] for e in permutations]
  edges_target = [e[1] for e in permutations]
  pres_edges = np.column_stack([edges_source, edges_target])
  all_edges = np.vstack([all_edges, pres_edges])

edge_index = all_edges.transpose()
edge_index = edge_index.astype(np.int32)

# create the graph, add the nodes and the edges
g = dgl.DGLGraph()
g.add_nodes(29)
src, dst = edge_index[0], edge_index[1]

# edges are direction in DGL so I make them bidirectional
g.add_edges(src, dst)
g.add_edges(dst, src)

# set the matrix that will contain the correlation between nodes
matrix_all = np.empty((g.number_of_nodes(), g.number_of_nodes()))

# retrieve the adjancecy matrix
matrix = g.adjacency_matrix()
matrix = matrix.to_dense()

# measure pearson correlation
n_nodes = g.number_of_nodes()
for i in range(n_nodes):
    for j in range(n_nodes):
        q = pearsonr(matrix[i],matrix[j])[0]
        matrix_all[i][j] = q

# compute kmean cluster, 5 cluster bc there are 5 president
kmeans = KMeans(n_clusters=5, random_state=random_seed).fit(matrix_all)

#Distribution of lables
plt.hist(kmeans.labels_)
plt.ylabel('Amount');
plt.xticks([0,1,2,3,4])
plt.show()

# Write cluster ids into corpus and X_df_gnn
df_gnn['cluster_id'] = kmeans.labels_
display(df_gnn.head())
X_df_gnn['cluster_id'] = kmeans.labels_
display(X_df_gnn.head())

# Number of documents in each cluster
df_gnn['cluster_id'].value_counts()

# number of document for every president
df_gnn['President'].value_counts()

# Principal Component Analysis which reduces the dimensionality of a data to an arbitrary number while preserving most of the information contained in it
pca_g = PCA(n_components=2, random_state=random_seed)

pca_vecs_g = pca_g.fit_transform(matrix_all)
pca_center_g = pca_g.fit_transform(kmeans.cluster_centers_)

# plot the document and the center

LABEL_COLOR_MAP ={0: 'blue',
                  1: 'green',
                  2: 'yellow',
                  3: 'purple',
                  4: 'black'}

label_color = [LABEL_COLOR_MAP[l]  for l in kmeans.labels_]

plt.scatter(pca_vecs_g[:,0], pca_vecs_g[:,1], c=label_color)
plt.scatter(pca_center_g[:,0], pca_center_g[:,1], c='red')
plt.show()

"""## 4 Word analysis"""

import wordcloud

# 5 Most frequent words in each cluster

cols = [c for c in X_df_gnn.columns if c!='cluster_id']

for c in df_gnn['cluster_id'].value_counts().index:
    print("CLUSTER ", c)
    print(X_df_gnn.loc[X_df_gnn['cluster_id']==c,cols].mean(axis=0).sort_values(ascending=False)[0:5])
    print('-----------')

# Word cloud for the 5 clusters
wd = wordcloud.WordCloud()
for c in df_gnn['cluster_id'].value_counts().index:
    print("CLUSTER ", c)
    texts = " ".join(df_gnn.loc[df_gnn['cluster_id']==c,'Clean'])
    cloud = wd.generate(texts)
    plt.imshow(cloud)
    plt.show()
    print('-----------')

"""## 5 Count vs Time

From wordcloud we can see that a word can have variations
"""

print(np.unique(list(filter(lambda x: x.startswith("americ"), df_gnn.Clean[0].split()))))

"""So we can search for a group of words, instead that 1 word and miss the others"""

# prepare the dataset with the count of the words for year

df_cont_gnn = pd.DataFrame()
df_cont_gnn['Year'] = df_gnn['Year']
df_cont_gnn['cont'] = 0
df_cont_gnn.cont = df_cont_gnn.cont.astype(int)
df_cont_gnn.head()

# fo every document assign the color of the party

colorsValue = []
for yy in df_cont_gnn.Year:
  if (yy > 1988 and yy < 1993) or (yy > 2000 and yy < 2009) or (yy > 2016):
    colorsValue.append('red')
  else:
    colorsValue.append('blue')

"""### 5.1 AMERICA and its variants"""

# count the occurrences of the words in the speech

def count_words(words):

  for i in range(df_gnn.shape[0]):
    cont = 0
    for w in words:
      cont += df_gnn.loc[i, 'Clean'].count(w)
    year = df_gnn.loc[i, 'Year']
    df_cont_gnn.cont[df_cont_gnn.Year== year] = int(cont)

americas = ['america', 'americas', 'american', 'americans']
count_words(americas)
df_cont_gnn.head()

plt.bar(x=df_cont_gnn.Year, height=df_cont_gnn.cont, color=colorsValue)
plt.xlabel('Year')
plt.ylabel('No of occurences')
plt.show()

"""### 5.2 TAX and its variants"""

taxes = ['tax','taxes','taxpayer','taxpayers']
count_words(taxes)
df_cont_gnn.head()

plt.bar(x=df_cont_gnn.Year, height=df_cont_gnn.cont, color=colorsValue)
plt.xlabel('Year')
plt.ylabel('No of occurences')
plt.show()

"""### 5.3 JOB and its variants"""

jobs = ['job', 'jobs']
count_words(jobs)
df_cont_gnn.head()

plt.bar(x=df_cont_gnn.Year, height=df_cont_gnn.cont, color=colorsValue)
plt.xlabel('Year')
plt.ylabel('No of occurences')
plt.show()

"""### 5.4 Result

Like in the R version

AMERICA and its variant it's more used by Democrats

TAX is also mostly Democrats and the falls during the first Clinton and Bush Jr from around 30 to 10 are strange

JOB is theme more addressed by Democrats than Republican too

It's interesting to see the change each time Democrats and Republicans switch

## 6 Error
"""

from sklearn.metrics import silhouette_score

"""Tota sum of square error"""

kmeans.inertia_

"""Silhouette index"""

silhouette_gnn = silhouette_score(matrix_all, kmeans.labels_, metric='euclidean')
print(silhouette_gnn)

"""We can see that SSE and silhouette index are much worse than the one computed in R and slightly worse than the machine learning one

# Recap Error

I show the error of the 3 model
"""

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# clus$tot.withinss

# Commented out IPython magic to ensure Python compatibility.
# %%R
# 
# mean(ss[,3])

# ML
print(clustering.inertia_, ' ', silhouette_ml)

# GNN
print(kmeans.inertia_, ' ', silhouette_gnn)

